Prototipo del Sistema de generación automática musical (SGAM)

Primer aproximación:

1. Segmentar audio en fragmentos de un mismo tamaño (2 segundos)
    comprobar si esto tiene sentido o sería mejor analizar desde los segmentos basados en MFCCs
2. Extraer las características de los segmentos
3.a Generar base de datos anotada con MSC
3.b Generar json con el nombre del segmento + (descriptores) + clase
4. Generar constructor en tiempo-real: 
    4.1 Grabar audio y segmentarlo
    4.2 Extraer caracteristicas
    4.3 Hacer predicción a futuro con el modelo LSTM 
    4.4 Reproducir archivo objetivo
    4.5 Escuchar lo reproducido para continuar indefinidamente con el bucle
    4.6 Generar pieza de x duración con Non-RTA 

Lo que espero es que genere composiciones coherentes a nivel formal en términos tímbricos, en el mejor de los casos generaría replicas o variaciones de las obras originales. 

¿porqué no tendría sentido?

Por que puede ser que los ejemplos generados no den cuenta claramente de cómo se desarrolla la forma musical.

¿Cómo extrapolar esto a el control de parámetros globales de las improvisaciones?

A través del control de amplitud, densidad de los materiales, brillo espectral (ecualización)

¿Cómo extrapolar esto al reconocimiento de la forma global de las improvisaciones?
